{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 189: EEG-Based Brain Computer Interfaces\n",
    "## Assignment 2: P300 Speller Classification via Linear Discriminant Analysis\n",
    "Created By: Alessandro \"Ollie\" D'Amico\n",
    "***\n",
    "**Please refer to Gradescope/Canvas for assignment deadlines.**\n",
    "***\n",
    "## Overview:\n",
    "For this assignment (tutorial), we will be working with a BCI competition dataset, with the primary goal of classifying target vs non-target trials from a P300 speller. The data we will be using comes from the 2nd Berlin BCI Competition, and was provided by Wadsworth Center, NYS Department of Health (Jonathan R. Wolpaw). Please go to the following link: \n",
    "http://www.bbci.de/competition/ii/\n",
    "<br><br>\n",
    "We will be working with **Dataset IIb**. Please click on the description of this dataset (pdf format is superior) and read through the page in order to answer the following questions\n",
    "Here is a direct link: http://www.bbci.de/competition/ii/albany_desc/albany_desc_ii.pdf\n",
    "***\n",
    "***Question 1:*** What is the *dt* (in ms; round to the nearest ms) for the row/column intensification? \n",
    "<br>*(Hint: Remember that from our last assignment, the dt at 1000 Hz = 1ms, 500 Hz = 2ms, 200 Hz = 5ms)*<br>\n",
    "***Question 2:*** How long (in ms) was each row/column intensified?<br>\n",
    "***Question 3:*** How much time (in ms) was there between each row/column intensification?<br>\n",
    "***Question 4:*** What is the sampling rate of the EEG data? **(DO NOT INCLUDE Hz IN YOUR ANSWER)**<br>\n",
    "***Question 5:*** Within a single block of 12 trials, how many times is each character illuminated?\n",
    "***\n",
    "<img src=\"assets/P300Matrix.gif\">\n",
    "\n",
    "*I created the animation following the description of the dataset, although this was not what participants actually saw. As you can tell, these stimuli are being presented in rapid succession. The participant was instructed to only focus on one character at a time. This shows 15 blocks of 12 intensifications which was the number of trials per letter described in the description*\n",
    "***\n",
    "***Note:*** The description of this dataset does not include the reference used. It took quite a long time to find, but I believe the reference used was the right earlobe as described in this paper:<br> https://hal.archives-ouvertes.fr/file/index/docid/521054/filename/krusienski_jne_2006.pdf\n",
    "<br>\n",
    "This is the same group and the parameters for data collection seem identical which is why I believe the descriptions of that paper apply to the data we'll be working with. They also mention they have already filtered their data with a 0.1 - 60 Hz bandpass filter. We will still apply a bandpass filter with a 0.1 Hz cut off just to be safe.\n",
    "***\n",
    "The owners of this data provided us with useful descriptions of the data, along with some matlab code. We will use their documentation to epoch the data so that we can train our classifier. We will be using linear discriminant analysis (LDA) for this problem, as it performs very well on P300 paradigms. Furthermore, this algorithm is rather intuitive. We will be using ```sklearn``` as it will make this assignment easier to follow.\n",
    "\n",
    "## Section 1: Setup\n",
    "Before we can begin processing our data, we must first import some useful packages. <br>\n",
    "- ```numpy``` is used widely for processing numerical data, and supports matrix operations natively. We will be using NumPy arrays to store our data.\n",
    "- ```scipy``` contains useful functions to calculate filter coefficients and carry out filtering. We will be using SciPy filters to clean our EEG data.\n",
    "- ```matplotlib``` is used to create plots. We will use it to visually explore our filter and EEG data.\n",
    "- ```sklearn``` used for machine learning. We will use this to train our classifier and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      \n",
    "import matplotlib.pyplot as plt                         \n",
    "import matplotlib.patches as patches\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz  \n",
    "from scipy.io import loadmat\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Pre-processing\n",
    "\n",
    "### Step 1: Design a filter\n",
    "The first thing I did, after creating some variables to store our sampling rate, dt, and number of channels, was to create a filter. I used the filter design from the previous assignment, a 0.1 Hz highpass, and a 30 Hz lowpass third-order Butterworth filter. Just like the previous assignment, we'll ```filtfilt``` to filter the data.\n",
    "<br><br>\n",
    "We will create some variables to store important values regarding the start and end of our epochs, our sampling rate and our bandpass coefficients. The filter we will be using is a non-causal Butterworth filter. We will use a third-order filter, which due to the nature of using ```filtfilt``` makes it functionally a sixth-order filter since the signal is being filtered twice. Optimal filter design is well beyond the scope of this course, but in general, low order non-causal IIR filters are commonly used for EEG pre-processing. However, IIR filters are easily made unstable so great care must be taken when choosing the order and coefficients. All coefficients chosen in this case have been tested for stability.\n",
    "<br><br>\n",
    "Here is the summary of our filter design:\n",
    "- Filter........: Butterworth (IIR), non-causal ```filtfilt```\n",
    "- Order.......: 3rd (functionally 6th via ```filtfilt```)\n",
    "- High pass: 0.1 Hz (low cut of our bandpass)\n",
    "- Low pass.: 30 Hz (high cut of our bandpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our filter variables\n",
    "fs = 240.0                      # Hz; sampling rate\n",
    "dt = 1000. / fs                 # ms; time between samples\n",
    "sdt = np.round(dt).astype(int); # rounded dt so that we can index samples\n",
    "hp = 0.1                        # Hz; our low cut for our bandpass\n",
    "lp = 30.                        # Hz; our high cut for our bandpass\n",
    "order = 3                       # filter order (functionally doubled)\n",
    "\n",
    "# Create our filter coefficient as as a second-order section\n",
    "# Note: by defining 'fs' we don't divide our windows by the Nyquist\n",
    "sos = butter(order, [hp, lp], analog = False, btype = 'band', output = 'sos', fs = fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Determine our epoch, baseline, and window of focus\n",
    "For the epoch, we'll plot the activity from 0ms (stimulus onset) to 700 ms (after onset) to ensure we have a waveform similar to the plot provided by the authors. For the baseline, we'll use 0ms to 100 ms after stimulus onset. This baseline window was chosen because every epoch contains information from the previous stimulus due to the small interstimulus interval (ISI). For the window we will select our ERP from, we'll use 200 ms after the stimulus to 400 ms after the stimulus. This choice is consistent with the theoretical expected onset of the P300, and minimizes signals from previous and subsequent stimuli. For readibility:\n",
    "- Epoch...: 0 - 700 ms\n",
    "- Baseline: 0 - 100 ms\n",
    "- ERP.......: 200 - 400 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ERP-related variables\n",
    "epoch_start = 0    # ms\n",
    "epoch_end = 700    # ms\n",
    "baseline_start = 0 # ms\n",
    "baseline_end = 100 # ms\n",
    "erp_start = 200    # ms\n",
    "erp_end = 400      # ms\n",
    "\n",
    "# Let's translate these into index space to save time later\n",
    "e_s = np.round(epoch_start / sdt).astype(int)     # epoch start\n",
    "e_e = np.round(epoch_end / sdt).astype(int)       # epoch end\n",
    "bl_s = np.round(baseline_start / sdt).astype(int) # baseline start\n",
    "bl_e = np.round(baseline_end / sdt).astype(int)   # baseline end\n",
    "erp_s = np.round(erp_start / sdt).astype(int)     # ERP start\n",
    "erp_e = np.round(erp_end / sdt).astype(int)       # ERP end\n",
    "\n",
    "# We'll also create our channel labels here since the document only provided a figure\n",
    "channels = np.array(['FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6',\n",
    "                     'C5',  'C3',  'C1',  'Cz',  'C2',  'C4',  'C6',\n",
    "                     'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6',\n",
    "                     'Fp1', 'Fpz', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8',\n",
    "                     'F7',  'F5',  'F3',  'F1',  'Fz',  'F2',  'F4',  'F6', 'F8',\n",
    "                     'FT7', 'FT8', 'T7',  'T8',  'T9',  'T10', 'TP7', 'TP8', \n",
    "                     'P7',  'P5',  'P3',  'P1',  'Pz',  'P2',  'P4',  'P6', 'P8',\n",
    "                     'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'O1',  'Oz',  'O2', 'Iz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Epoch our data\n",
    "\n",
    "We are now ready to extract our signals of interest. All of our data is contained in the *data* subfolder within this directory. Inside are 19 ```.mat``` files. These files were all recorded from the same subject performing unique runs of copy spelling. Copy spelling is when a subject is told what words/letters they will be spelling so that we can label their data and train a classifier to be used during free spelling. In this case, as mentioned in the document, our training data have the prefix ***AAS010*** and ***AAS011*** , while our test (unlabeled) data have the prefix ***AAS012***\n",
    "<br><br>\n",
    "Using our previously defined epoch-related and filter-related variables, we will iterate through all of our files, load them in, extract the relevant pieces of information, process our signals, and store them in easily accessed structures. Our training data, ```train_data```, will also be accompanied by ```train_labels``` which will be a binary vector with 0 indicating non-target and 1 indicating a target, and ```train_markers```, which will contain the row/column which was illuminated during the trial. For our testing data, ```test_data``` will contain our epochs of interest, but since we have no labels for this data, it will only be accompanied by ```test_markers```.\n",
    "<br><br>\n",
    "It's important to note that we will be filtering our EEG signals as we append them to their respective structures.\n",
    "<br><br>\n",
    "***Since the code below is written to be maximally readable, it will be relatively slow.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store our data. We'll conver them into np.arrays at the end\n",
    "train_data = []\n",
    "train_labels = []\n",
    "train_markers = []\n",
    "test_data = []\n",
    "test_markers = []\n",
    "\n",
    "# Load all data in (concatenate train data, keep test separate)\n",
    "pfx = 'AAS0'\n",
    "dir_ = './data/'\n",
    "train = [10, 11]\n",
    "S = [10, 11, 12]\n",
    "R = [x for x in range(1, 9)]\n",
    "for s in S:\n",
    "    for r in R:\n",
    "        fp = Path(dir_ + pfx + '%dR0%d.mat' % (s, r))\n",
    "        if fp.exists() and fp.is_file():\n",
    "            # If file exists, load, process, append\n",
    "            print('Processing: ' + str(fp))\n",
    "            x = loadmat(fp)\n",
    "\n",
    "            # All we need are the EEG data (signal) and markers (StimulusCode) out of this\n",
    "            data_   = np.array(x['signal'])        # dim (num(time_points) x num(channels))\n",
    "            mrks_   = x['StimulusCode'].flatten(); # dim (num(time_points) x 1)\n",
    "\n",
    "            # Create a stimulus onset array\n",
    "            onsets = np.zeros_like(mrks_)\n",
    "            for i in range(mrks_.shape[0]):\n",
    "                # Roughly equivalent to positive first derivative, but easier to read\n",
    "                if mrks_[i] != 0 and mrks_[i-1] == 0:\n",
    "                    onsets[i] = 1\n",
    "                    \n",
    "            # Determine all indices where onsets occur\n",
    "            onsets_ix = np.where(onsets == 1)[0]\n",
    "        \n",
    "            # Iterate through onsets, epoch, filter, append\n",
    "            for i in range(len(onsets_ix)):\n",
    "                t0 = onsets_ix[i]\n",
    "                \n",
    "                # Epoch and correct DC offset of signal\n",
    "                data = data_[e_s+t0:t0+e_e, :] - np.mean(data_[e_s+t0:t0+e_e, :], 0)\n",
    "                \n",
    "                # Filter the epoch\n",
    "                data = sosfiltfilt(sos, data)\n",
    "                \n",
    "                # Now let's baseline correct\n",
    "                data = data - np.mean(data[bl_s+np.abs(e_s):np.abs(e_s)+bl_e, :], 0)\n",
    "                \n",
    "                # Append data to correct locations\n",
    "                if s in train:\n",
    "                    train_data.append(data)\n",
    "                    train_markers.append(mrks_[t0])\n",
    "                    train_labels.append(x['StimulusType'].flatten()[t0]) # target or non-target\n",
    "                else:\n",
    "                    test_data.append(data)\n",
    "                    test_markers.append(mrks_[t0])\n",
    "                    \n",
    "print('Processing completed!')\n",
    "\n",
    "# Convert all of our lists into numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "train_markers = np.array(train_markers)\n",
    "test_data = np.array(test_data)\n",
    "test_markers = np.array(test_markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our epoched signals, let's divide our ```train_data``` into target and non-target categories for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target and non-target for plotting\n",
    "tar     = train_data[np.where(train_labels == 1)[0], :, :]\n",
    "non_tar = train_data[np.where(train_labels == 0)[0], :, :]\n",
    "\n",
    "print('We have %d target trials' % tar.shape[0])\n",
    "print('We have %d non-target trials' % non_tar.shape[0])\n",
    "\n",
    "# We'll take the average of all trials to create an averaged ERP\n",
    "tar_avg     = np.mean(tar, 0)\n",
    "non_tar_avg = np.mean(non_tar, 0)\n",
    "\n",
    "# Define channel of interest and create an array of time points\n",
    "chan = 'Pz' # let's plot Pz\n",
    "ch = np.where(channels == chan)[0][0]\n",
    "times = np.linspace(epoch_start, epoch_end, train_data.shape[1])\n",
    "\n",
    "# Initialize plot and calculate min and max y value\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n",
    "min_y = min(np.min(tar_avg), np.min(non_tar_avg))\n",
    "max_y = max(np.max(tar_avg), np.max(non_tar_avg))\n",
    "\n",
    "# Plot x and y axes\n",
    "plt.plot([np.min(times), np.max(times)], [0, 0], color='k');  # x-axis\n",
    "plt.plot([0, 0], [min_y, max_y], color='k');                  # y-axis\n",
    "\n",
    "# Plot our averaged ERPs\n",
    "plt.plot(times, tar_avg[:, ch], 'b', linewidth=4)\n",
    "plt.plot(times, non_tar_avg[:, ch], 'r', linewidth=4)\n",
    "\n",
    "# Highlight the baseline window and window of interest of our ERP\n",
    "baseline = patches.Rectangle([baseline_start, min_y], baseline_end, np.abs(min_y)+max_y, \n",
    "                             color='c', alpha=0.2)\n",
    "erp_win = patches.Rectangle([erp_start, min_y], erp_end-erp_start, np.abs(min_y)+max_y, \n",
    "                             color='lime', alpha=0.3)\n",
    "\n",
    "# Add our baseline and window of interest highlights\n",
    "ax.add_patch(baseline)\n",
    "ax.add_patch(erp_win)\n",
    "\n",
    "# Manually create legends since patches will corrupt default handles\n",
    "legend_ = [patches.Patch(color='b', label = 'Target (oddball)'),\n",
    "           patches.Patch(color='r', label = 'Non-target (standard)')]\n",
    "\n",
    "# Finalize plot and set a high DPI for a crisp, hi-res figure\n",
    "plt.xlabel('Time (msec)');\n",
    "plt.ylabel('Amplitude (A/D Units)');\n",
    "plt.legend(handles=legend_, loc=\"upper right\");\n",
    "plt.title('Event Related Potentials at channel %s' % chan);\n",
    "fig.set_dpi(216);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Determine how many signals to extract**<br>\n",
    "There is a lot of mutual information in our data, and it would not be productive to export all of it, specifically because it would take more time to train/cross-validate our model. If you're curious, try modifying ```channel``` to examine the ERPs at different locations. You'll find that there are a lot of simillarities between the waveforms of neighboring channels. Neighboring time points are also very similar and we could faithfully recreate our waveform using significantly fewer samples. With this in mind, we can reduce our dimensionality quickly by sub-sampling our data in time, while preseving all 64 channels. One efficient way to sub-sample in a scenario like this is to calculated the ***windowed means*** of our signal of interest.\n",
    "<br><br>\n",
    "<img src=\"assets/img1.png\">\n",
    "<center>A simple illustration of sub-sampling (more generalized than windowed means)</center>\n",
    "<br><br>\n",
    "To calculate our windowed means, we can divide our data into equally sized chunks and calculate the means. In some instances it can be beneficial to have overlap between windows, but for this case we can keep the windows totally seperated. Let's extract 5 points per waveform per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the windowed means within erp_start and erp_end\n",
    "num_points = 5; # we will divide our window into num_points means\n",
    "\n",
    "# Define a simple windowed means function\n",
    "def wm(x, start, end, num_points):\n",
    "    num_trials = x.shape[0] # assumes first dem is numb observations\n",
    "    w = np.round((start+end)/num_points).astype(int)\n",
    "    y = np.zeros((num_points, x.shape[-1], num_trials)) # assumes num chans as last dimension\n",
    "    for i in range(0, num_points):\n",
    "        s = start + (w * i)\n",
    "        e = end   + (w * i)\n",
    "        y[i, :, :] = np.mean(x[:, s:e, :], 1).T\n",
    "    return y\n",
    "\n",
    "# Combine into a single train variable. Also create labels\n",
    "X_train    = wm(train_data, erp_s, erp_e, num_points)\n",
    "markers_train = np.vstack((train_labels, train_markers)).T\n",
    "y = train_labels\n",
    "\n",
    "# Now let's compute windowed means of our test data\n",
    "X_test = wm(test_data, erp_s, erp_e, num_points)\n",
    "markers_test = test_markers\n",
    "\n",
    "# Let's print out the shapes of our data\n",
    "print('X_train shape is: ' + str(X_train.shape))\n",
    "print('y shape is......: ' + str(y.shape))\n",
    "print('X_test shape is.: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dimensions of ```X_train``` are 5 x 64 x 7561. We have 5 samples per observation (since we 10-times subsample a window of ~50 samples), 64 channels, and 7561 unique observations. Similarly, we have 5580 unique observations for our test data, ```X_test```. We must restructure our data so that they can be passed into our classifier. Luckily, our 5 x 64 features can be flattened, and we can subsequently transpose our matrix so that the first dimension matches our label vector (this is the format ```sklearn``` expects). We will do the same flattening and transposing on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our X is 3D, we must flatten our data. We will then transpose it for sklearn\n",
    "X_train = X_train.reshape(-1, X_train.shape[-1]).T\n",
    "X_test = X_test.reshape(-1, X_test.shape[-1]).T\n",
    "\n",
    "# Let's print out the new shape\n",
    "print('X_train shape is now: ' + str(X_train.shape))\n",
    "print('X_test  shape is now: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data are correctly formatted, we can train a model! We will first choose our classifier. One standard choice for classifying P300 data is via linear discriminant analysis (LDA) which has been discussed in class. sklearn supports LDA (and its quadratic counterpart QDA) which makes the process rather simple. \n",
    "<br><br>\n",
    "We will be using the least-squares solver, which tends to be more efficient for higher dimensional feature vectors.\n",
    "<br><br>\n",
    "We will be using shrinkage, which is important considering how much data we have, and we will determine this parameter automatically.<br>\n",
    "See: https://scikit-learn.org/stable/auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our classifier (this may take a while via JupyterHub)\n",
    "clf_lsqrs = LinearDiscriminantAnalysis(solver = 'lsqr',  shrinkage = 'auto').fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 5-fold cross validation to determine the accuracy of our model<br>\n",
    "***Note: This may take a few minutes to run on slower machines***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do 5-fold cross validation\n",
    "score_lsqrs = cross_val_score(clf_lsqrs.fit(X_train, y), X_train, y, cv = 5)\n",
    "\n",
    "# We will print out the mean score\n",
    "print(\"solver = lsqr  accuracy: %f\" % np.mean(score_lsqrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the non-target stimuli are considerably more common *(10/12, 83%)*, we would expect to get around 80% accuracy by just saying everything is a non-target. In order to examine our model more carefully, we will use the receiver operating characteristic curve (ROC) and calculate the area under the curve (AUC) to see how often our classifier is correctly labeling the data. We will also use 5-fold cross validation for this method.<br>\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic <br>\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/ <br>\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "nfolds = 5;\n",
    "cv = StratifiedKFold(n_splits = nfolds)\n",
    "X = X_train\n",
    "tprs_lsqrs = [];\n",
    "aucs_lsqrs = [];\n",
    "mean_fpr_lsqrs = np.linspace(0, 1, 100);\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    probas_lsqrs = clf_lsqrs.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    fpr_lsqrs, tpr_lsqrs, th_lsqrs = roc_curve(y[test], probas_lsqrs[:, 1])\n",
    "    tprs_lsqrs.append(np.interp(mean_fpr_lsqrs, fpr_lsqrs, tpr_lsqrs))\n",
    "    tprs_lsqrs[-1][0] = 0.0\n",
    "    roc_auc_lsqrs = auc(fpr_lsqrs, tpr_lsqrs)\n",
    "    aucs_lsqrs.append(roc_auc_lsqrs)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our ROC and calculated the AUC, let's plot the curve and include the AUCs for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr_lsqrs = np.mean(tprs_lsqrs, axis=0)\n",
    "mean_tpr_lsqrs[-1] = 1.0\n",
    "mean_auc_lsqrs = auc(mean_fpr_lsqrs, mean_tpr_lsqrs)\n",
    "std_auc_lsqrs = np.std(aucs_lsqrs);\n",
    "\n",
    "plt.plot(mean_fpr_lsqrs, mean_tpr_lsqrs, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc_lsqrs, std_auc_lsqrs),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr_lsqrs = np.std(tprs_lsqrs, axis=0)\n",
    "tprs_upper_lsqrs = np.minimum(mean_tpr_lsqrs + std_tpr_lsqrs, 1)\n",
    "tprs_lower_lsqrs = np.maximum(mean_tpr_lsqrs - std_tpr_lsqrs, 0)\n",
    "plt.fill_between(mean_fpr_lsqrs, tprs_lower_lsqrs, tprs_upper_lsqrs, color='b', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC of Least-Squares LDA (' + str(nfolds) + '-fold CV)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the AUCs agree with our 5-fold cross validation above. I ran this analysis with 24-fold cross validation, and the AUC was 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set clf to clf_lsqrs for convenience\n",
    "clf = clf_lsqrs.fit(X_train, y)\n",
    "\n",
    "# Make predictions on our dataset \n",
    "conf = clf.decision_function(X_test) # predicted confidence score\n",
    "pred = clf.predict(X_test)           # predicted label (we won't actually use this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have trained a model, and used it to make predictions on individual observations (target vs non-target). We will be using the confidence score of each classification to now determine which character the subject was attending to. The larger the score, the more confident the classification was that the observation was a flashed target.\n",
    "<br><br>\n",
    "Let's create a numpy array to mimic the character matrix described in the data description, in order to efficiently determine which character our classifier predicted\n",
    "<img src=\"assets/img2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the translations for the labels:\n",
    "# Columns 0-5 -> Labels 1-6\n",
    "# Rows 0-5 -> Labels 7-12\n",
    "# Let's create this matrix\n",
    "#                         1    2    3    4    5    6\n",
    "char_matrix = np.array([['A', 'B', 'C', 'D', 'E', 'F'], # 7\n",
    "                        ['G', 'H', 'I', 'J', 'K', 'L'], # 8\n",
    "                        ['M', 'N', 'O', 'P', 'Q', 'R'], # 9\n",
    "                        ['S', 'T', 'U', 'V', 'W', 'X'], # 10\n",
    "                        ['Y', 'Z', '1', '2', '3', '4'], # 11\n",
    "                        ['5', '6', '7', '8', '9', '0']])# 12 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for each character, each row and column was flashed once, for a total of 12 flashes per block and this was repeated 15 times. This means that the first 12x15 data points correspond to the first character, the 2nd 12x15 data point correspond to the second character, etc.\n",
    "<br><br>\n",
    "Let's examine the first character from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful variables\n",
    "char_breaks = 15 * 12 # number of stimuli per character\n",
    "c = 1;                # which character we want\n",
    "\n",
    "# Grab our first character's marker, prediction, and confidence\n",
    "char1_mrks = markers_test[char_breaks*(c-1):char_breaks*c]\n",
    "char1_pred = pred[char_breaks*(c-1):char_breaks*c]\n",
    "char1_conf = conf[char_breaks*(c-1):char_breaks*c]\n",
    "\n",
    "# Let's merge these\n",
    "char1 = np.array([char1_mrks, char1_pred, char1_conf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an array *char1* which contains 180 stimuli, their marker (which row/col), their prediction (0, 1 unused), and the confidence of the prediction (float). Let's sort our char1 by confidence in order to determine which row and column gives us the character that we believe to be the subject's target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, now let's sort them by the 3rd column, the confidence\n",
    "# Nice 1-liner provided by https://stackoverflow.com/questions/2828059/sorting-arrays-in-numpy-by-column\n",
    "char1_sort = char1[char1[:,2].argsort()[::-1]]\n",
    "\n",
    "# Let's print out the first 5 results\n",
    "print(char1_sort[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the flashes, the one with the highest confidence (that it contains a target) is the marker '7' which corresponds to the first row.<br>\n",
    "Our second highest confidence is the marker '6' which corresponds to the last column.<br> \n",
    "We could simply take the column/row that has the highest single-flash confidence and consider the target character prediction to be the character in that highest confidence row and highest confident column. However, it's possible that those values are outliers. In order to make better use of all the data, we will take the mean of every marker's (row's and column's) confidence, and use the row and column with the highest average confidence to determine our character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will store the average confidence value for every row/column\n",
    "x = np.zeros(shape = (12, 1))\n",
    "for i in range(0, x.shape[0]):\n",
    "    x[i] = np.mean(char1_sort[char1_sort[:, 0] == i + 1, 2])\n",
    "    \n",
    "# Now let's print our character\n",
    "row = np.argmax(x[6:])\n",
    "col = np.argmax(x[0:6])\n",
    "our_char = char_matrix[row, col]\n",
    "print(f'Our character is: {our_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly visualize the average confidence scores with a bar plot\n",
    "plt.figure()\n",
    "ticks = np.array(range(1, x.shape[0] + 1, 1))\n",
    "plt.bar(ticks, np.ravel(x), tick_label = ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Our classifier predicts that the first character the subject attended to was the letter 'F'. Let's now use this logic and apply it iteratively to each character in the test data, and print out every character the user attended to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: Iterate through test data by chunking every 15x12 trials per character\n",
    "#  Nest 1: Calculate mean of character\n",
    "char_breaks = 15*12\n",
    "output_string = f''\n",
    "for c in range(1, int(X_test.shape[0] / char_breaks) + 1):\n",
    "    # Lets break up test into char 'c'\n",
    "    temp_mrks = markers_test[char_breaks * (c - 1):char_breaks * c]\n",
    "    temp_pred = pred[char_breaks * (c - 1):char_breaks * c]\n",
    "    temp_conf = conf[char_breaks * (c - 1):char_breaks * c]\n",
    "\n",
    "    # Let's merge these and sort by highest confidence\n",
    "    temp_char = np.array([temp_mrks, temp_pred, temp_conf]).T\n",
    "    temp_char = temp_char[temp_char[:, 2].argsort()[::-1]]\n",
    "    \n",
    "    # Create our averages\n",
    "    x = np.zeros(shape = (12, 1))\n",
    "    for i in range(0, x.shape[0]):\n",
    "        x[i, 0] = np.mean(temp_char[temp_char[:, 0] == i + 1, 2])\n",
    "        \n",
    "    # Now let's print our character\n",
    "    row = np.argmax(x[6:,  0])\n",
    "    col = np.argmax(x[0:6, 0])\n",
    "    our_char = char_matrix[row, col]\n",
    "    output_string += f'{our_char} '\n",
    "    \n",
    "# Print our result\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a competition dataset, we were obviously not given the labels to our test set. However, since this competition is very old (2002), the results are public!\n",
    "<br><br>\n",
    "5 teams got first place in this competition, each with test character 100% accuracy. The added challenge was to minimize the amount of flashes each character was required, but for simplicity we will not do this.\n",
    "<br><br>\n",
    "Since all of these teams were able to determine the desired characters with 100% accuracy, we can look at one of their write-ups to determine the true labels of the test set.<br>\n",
    "The approach we took for this assignment was influenced by: http://www.bbci.de/competition/ii/results/tax_iib_desc.pdf\n",
    "<br>\n",
    "***\n",
    "***Question 6:*** What percentage of our test characters were correctly classified?\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
